# XZatoma Configuration File
#
# This file configures the XZatoma autonomous AI agent.
# All settings can be overridden via environment variables or CLI arguments.

provider:
  # Provider type: "copilot" or "ollama"
  type: ollama

  # GitHub Copilot configuration
  copilot:
    # Model to use (e.g., gpt-5.3-codex, claude-sonnet-4.6)
    model: gpt-5.3-codex

  # Ollama configuration
  ollama:
    # Ollama server host
    host: http://localhost:11434
    # Model to use (e.g., llama3.2:3b, granite3.2:2b, granite4:3b)
    model: llama3.2:3b

agent:
  # Maximum number of agent turns before stopping
  max_turns: 50

  # Timeout for entire agent execution (seconds)
  timeout_seconds: 300

  # Conversation management settings
  conversation:
    # Maximum tokens allowed in conversation context
    max_tokens: 100000
    # Minimum number of turns to retain when pruning
    min_retain_turns: 5
    # Token threshold to trigger pruning (0.0-1.0)
    prune_threshold: 0.8

    # Context window management
    warning_threshold: 0.85 # Warn at 85% full
    auto_summary_threshold: 0.90 # Auto-summarize at 90% full

    # Use cheaper model for summaries (optional)
    # summary_model: "gpt-5.1-codex-mini"

  # Tool execution settings
  tools:
    # Maximum size of tool output (bytes)
    max_output_size: 1048576
    # Maximum size of file to read (bytes)
    max_file_read_size: 10485760

  # Terminal execution settings
  terminal:
    # Execution mode: interactive, restricted_autonomous, full_autonomous
    default_mode: restricted_autonomous
    # Timeout for terminal commands (seconds)
    timeout_seconds: 30
    # Maximum stdout size (bytes)
    max_stdout_bytes: 1048576
    # Maximum stderr size (bytes)
    max_stderr_bytes: 262144

  # Chat mode settings
  chat:
    # Default chat mode: planning (read-only) or write (read/write)
    default_mode: planning
    # Default safety mode: confirm (always confirm) or yolo (never confirm)
    default_safety: confirm
    # Allow switching between modes during a session
    allow_mode_switching: true

  # Subagent configuration
  subagent:
    # Maximum recursion depth for nested subagents (1-10)
    max_depth: 3

    # Default maximum turns per subagent execution
    default_max_turns: 10

    # Maximum output size in bytes before truncation
    output_max_size: 4096

    # Enable subagent execution telemetry
    telemetry_enabled: true

    # Enable conversation persistence for debugging
    persistence_enabled: false

    # Optional: Override provider for subagents
    # If specified, subagents will use this provider instead of the parent provider
    # Valid values: "copilot", "ollama"
    # provider: copilot

    # Optional: Override model for subagents
    # If specified, subagents will use this model instead of the provider's default
    # Example: "gpt-5.1-codex-mini" for Copilot, "llama3.2:3b" for Ollama
    # model: gpt-5.1-codex-mini

    # Enable subagents in chat mode by default
    # If false (default), requires explicit prompt pattern or command to enable
    # If true, subagents are available immediately in chat mode
    chat_enabled: false

    # Maximum total subagent executions per session (optional)
    # max_executions: 100

    # Maximum total tokens for all subagents (optional)
    # max_total_tokens: 1000000

    # Maximum wall-clock time for all subagents in seconds (optional)
    # max_total_time: 3600
# MCP (Model Context Protocol) client configuration
# mcp:
#   auto_connect: true
#   request_timeout_seconds: 30
#   expose_resources_tool: true
#   expose_prompts_tool: true
#   servers:
#     - id: "my_stdio_server"
#       transport:
#         type: "stdio"
#         executable: "/usr/local/bin/my-mcp-server"
#         args: []
#         env: {}
#     - id: "my_http_server"
#       transport:
#         type: "http"
#         endpoint: "https://mcp.example.com/mcp"
#         oauth:
#           redirect_port: 8765
